"""
Cha√Æne RAG pour interroger les documents RGPD et IA ACT
Utilise ChromaDB pour la recherche et Ollama pour la g√©n√©ration
"""
import os
import warnings

# Disable telemetry and warnings
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY"] = "False"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

# Suppress all common warnings
warnings.filterwarnings("ignore", message="Failed to send telemetry event")
warnings.filterwarnings("ignore", category=UserWarning, module="torch")
warnings.filterwarnings("ignore", message=".*torch.classes.*")

from typing import List, Dict, Any
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains.llm import LLMChain
from langchain_core.prompts import PromptTemplate
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.documents import Document


class StreamingCallbackHandler(BaseCallbackHandler):
    """Callback handler pour le streaming des r√©ponses"""
    
    def __init__(self, container):
        self.container = container
        self.current_text = ""
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Appel√© √† chaque nouveau token g√©n√©r√©"""
        self.current_text += token
        # Met √† jour l'affichage en temps r√©el
        self.container.markdown(self.current_text)


class RAGChain:
    """Classe pour g√©rer la cha√Æne RAG"""
    
    def __init__(self, persist_directory: str = "./chroma_db", model: str = "gemma3:4b", num_sources: int = 5):
        """
        Initialise la cha√Æne RAG
        
        Args:
            persist_directory: R√©pertoire de la base vectorielle
            model: Nom du mod√®le Ollama √† utiliser
            num_sources: Nombre de sources √† r√©cup√©rer
        """
        self.persist_directory = persist_directory
        self.model_name = model
        self.num_sources = num_sources
        
        # Initialise les embeddings (m√™mes que lors de l'indexation)
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # Charge la base vectorielle
        self.vectorstore = self._load_vectorstore()
        
        # Initialise le LLM local
        self.llm = self._init_llm()
        
        # Cr√©e le retriever (pas de cha√Æne compl√®te, on g√®re manuellement pour les citations)
        self.retriever = self._create_qa_chain()
    
    def _load_vectorstore(self) -> Chroma:
        """Charge la base vectorielle existante"""
        print(f"üì¶ Chargement de la base vectorielle depuis {self.persist_directory}")
        vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings
        )
        print(f"‚úÖ Base vectorielle charg√©e")
        return vectorstore
    
    def _init_llm(self) -> OllamaLLM:
        """Initialise le mod√®le LLM local via Ollama"""
        print(f"ü§ñ Initialisation du mod√®le {self.model_name} via Ollama")
        llm = OllamaLLM(
            model=self.model_name,
            temperature=0.1,  # Temp√©rature basse pour plus de pr√©cision
            callbacks=[StreamingStdOutCallbackHandler()]
        )
        print(f"‚úÖ Mod√®le initialis√©")
        return llm
    
    def _create_prompt_template(self) -> PromptTemplate:
        """Cr√©e le template de prompt pour le LLM avec citations inline"""
        template = """Tu es un assistant expert en conformit√© juridique, sp√©cialis√© dans le RGPD (R√®glement G√©n√©ral sur la Protection des Donn√©es) et l'IA Act (r√®glement europ√©en sur l'intelligence artificielle).

R√àGLE ABSOLUE: R√©ponds PR√âCIS√âMENT √† la question pos√©e. Ne change pas de sujet.

Utilise UNIQUEMENT les informations du contexte ci-dessous pour r√©pondre √† la question. Si la r√©ponse n'est pas dans le contexte, dis clairement que tu ne peux pas r√©pondre avec certitude.

Contexte (extraits de documents juridiques):
{context}

Question: {question}

RAPPEL: Ta r√©ponse doit porter UNIQUEMENT sur: {question}

Instructions de r√©ponse IMPORTANTES:
- **STRUCTURE COMME UN EXECUTIVE SUMMARY** : Utilise des titres, sous-titres et une mise en forme claire
- R√©ponds de mani√®re naturelle et fluide, comme un expert juridique
- **CRITICAL: Cite les sources √† la FIN de chaque paragraphe, pas dans le texte**
- **NE mentionne JAMAIS les sources dans le texte** (√©vite "Le texte [Source X] dit que...")
- √âcris de mani√®re directe et naturelle, puis ajoute [Source X] √† la fin du paragraphe
- Tu peux combiner plusieurs sources dans un m√™me paragraphe: [Source 1] [Source 3]

STRUCTURE OBLIGATOIRE:
- Commence par un r√©sum√© en 2-3 phrases
- Utilise des **titres** (## Titre) pour organiser les sections principales
- Utilise des **sous-titres** (### Sous-titre) pour les d√©tails
- Fais des **listes √† puces** (-) quand c'est appropri√©
- Termine par une **conclusion** ou **recommandations** si pertinent
- Garde les paragraphes courts (3-4 phrases max)
- Cite les articles, chapitres et sections pertinents quand c'est possible
- Distingue clairement entre le RGPD et l'IA Act si les deux sont mentionn√©s
- Reste factuel et √©vite les interpr√©tations personnelles
- Si la r√©ponse n'est pas dans le contexte, dis-le clairement

Format des citations:
- Place [Source X] √† la fin de chaque paragraphe qui utilise cette source
- Pour plusieurs sources dans un paragraphe: [Source 1] [Source 2]
- IMPORTANT: Ne cite JAMAIS les sources au milieu d'une phrase, TOUJOURS √† la fin du paragraphe

Exemples de MAUVAIS style (NE PAS FAIRE):
‚ùå "Le texte [Source 7] stipule que..."
‚ùå "Selon [Source 8], le DPO doit..."
‚ùå "De plus, [Source 9] d√©crit..."

Exemples de BON style (√Ä SUIVRE):
‚úÖ "Le responsable du traitement doit mettre en ≈ìuvre des mesures appropri√©es. [Source 7]"
‚úÖ "Il doit superviser les activit√©s de traitement et √©valuer les risques. [Source 3]"
‚úÖ "Les obligations incluent la supervision continue et l'√©valuation. [Source 1] [Source 3]"

R√âPONDS UNIQUEMENT √Ä LA QUESTION POS√âE en utilisant le contexte fourni. Ne d√©vie pas du sujet. 

R√©ponse:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
    
    def _create_qa_chain(self):
        """
        Cr√©e une cha√Æne RAG personnalis√©e avec citations num√©rot√©es
        Note: Retourne juste le retriever, on g√®re manuellement la g√©n√©ration pour avoir plus de contr√¥le
        """
        print(f"‚õìÔ∏è  Cr√©ation de la cha√Æne RAG avec support de citations")
        
        # On va g√©rer manuellement la cha√Æne pour avoir un contr√¥le total sur le formatage
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": self.num_sources}
        )
        
        print(f"‚úÖ Retriever cr√©√© avec support de citations num√©rot√©es")
        return self.retriever
    
    def _format_docs_with_sources(self, docs: List[Document]) -> str:
        """
        Formate les documents avec des num√©ros de source pour les citations
        
        Args:
            docs: Liste de documents r√©cup√©r√©s
            
        Returns:
            String format√©e avec documents num√©rot√©s
        """
        formatted_docs = []
        for i, doc in enumerate(docs, 1):
            formatted_docs.append(f"[Source {i}]\n{doc.page_content}\n")
        return "\n".join(formatted_docs)
    
    def query(self, question: str) -> Dict[str, Any]:
        """
        Pose une question √† la cha√Æne RAG avec citations inline
        
        Args:
            question: La question √† poser
            
        Returns:
            Dictionnaire contenant la r√©ponse et les sources
        """
        print(f"\n{'='*60}")
        print(f"‚ùì Question: {question}")
        print(f"{'='*60}\n")
        
        # 1. R√©cup√®re les documents pertinents (API moderne LangChain)
        print(f"üîç Recherche de documents pertinents...")
        source_documents = self.retriever.invoke(question)
        print(f"‚úÖ {len(source_documents)} documents trouv√©s")
        
        # 2. Formate les documents avec num√©ros de source
        formatted_context = self._format_docs_with_sources(source_documents)
        
        # 3. Cr√©e le prompt avec contexte num√©rot√©
        prompt_template = self._create_prompt_template()
        prompt = prompt_template.format(context=formatted_context, question=question)
        
        # 4. G√©n√®re la r√©ponse avec le LLM
        print(f"ü§ñ G√©n√©ration de la r√©ponse avec citations...")
        answer = self.llm.invoke(prompt)
        
        return {
            "question": question,
            "answer": answer,
            "source_documents": source_documents
        }
    
    def query_streaming(self, question: str, container) -> Dict[str, Any]:
        """
        Pose une question √† la cha√Æne RAG avec streaming en temps r√©el
        
        Args:
            question: La question √† poser
            container: Container Streamlit pour l'affichage en temps r√©el
            
        Returns:
            Dictionnaire contenant la r√©ponse et les sources
        """
        print(f"\n{'='*60}")
        print(f"‚ùì Question (streaming): {question}")
        print(f"{'='*60}\n")
        
        # 1. R√©cup√®re les documents pertinents
        print(f"üîç Recherche de documents pertinents...")
        source_documents = self.retriever.invoke(question)
        print(f"‚úÖ {len(source_documents)} documents trouv√©s")
        
        # 2. Formate les documents avec num√©ros de source
        formatted_context = self._format_docs_with_sources(source_documents)
        
        # 3. Cr√©e le prompt avec contexte num√©rot√©
        prompt_template = self._create_prompt_template()
        prompt = prompt_template.format(context=formatted_context, question=question)
        
        # 4. Configure le LLM avec le callback de streaming
        streaming_callback = StreamingCallbackHandler(container)
        llm_with_streaming = OllamaLLM(
            model=self.model_name,
            temperature=0.1,
            callbacks=[streaming_callback]
        )
        
        # 5. G√©n√®re la r√©ponse avec streaming
        print(f"ü§ñ G√©n√©ration de la r√©ponse avec streaming...")
        answer = llm_with_streaming.invoke(prompt)
        
        return {
            "question": question,
            "answer": answer,
            "source_documents": source_documents
        }
    
    def extract_citations(self, answer: str) -> Dict[str, Any]:
        """
        Extrait les citations [Source X] de la r√©ponse et cr√©e un mapping
        
        Args:
            answer: R√©ponse g√©n√©r√©e par le LLM
            
        Returns:
            Dict avec la r√©ponse et les sources cit√©es
        """
        import re
        
        # Trouve toutes les citations [Source X] dans la r√©ponse
        citation_pattern = r'\[Source\s+(\d+)\]'
        citations = re.findall(citation_pattern, answer)
        
        # Convertit en set pour avoir les sources uniques, puis en liste tri√©e
        cited_sources = sorted(set(int(c) for c in citations))
        
        return {
            "answer": answer,
            "cited_sources": cited_sources,
            "total_citations": len(citations)
        }
    
    def format_sources(self, source_documents: List) -> List[Dict[str, str]]:
        """
        Formate les documents sources pour l'affichage avec m√©tadonn√©es enrichies
        
        Args:
            source_documents: Liste de documents sources
            
        Returns:
            Liste de dictionnaires format√©s avec informations structurelles
        """
        sources = []
        for i, doc in enumerate(source_documents):
            metadata = doc.metadata
            
            # Construit le titre du chunk bas√© sur la structure
            chunk_title = self._build_chunk_title(metadata)
            
            # Nettoie le contenu pour un meilleur affichage
            cleaned_content = self._clean_content(doc.page_content)
            
            # Extrait les informations de contexte
            context_info = self._extract_context_info(metadata)
            
            sources.append({
                "index": i + 1,
                "content": cleaned_content,
                "source_file": metadata.get("source_file", "Inconnu"),
                "page": metadata.get("page", "N/A"),
                "chunk_title": chunk_title,
                "content_type": metadata.get("content_type", "paragraph"),
                "article_number": metadata.get("article_number"),
                "chapter_title": metadata.get("chapter_title"),
                "section_title": metadata.get("section_title"),
                "key_terms": metadata.get("key_terms", []),
                "word_count": metadata.get("word_count", 0),
                "context_info": context_info,
                "chunk_quality": metadata.get("chunk_quality", 1.0),
                "is_complete": metadata.get("is_complete", True)
            })
        return sources
    
    def _build_chunk_title(self, metadata: dict) -> str:
        """
        Construit un titre descriptif pour le chunk
        
        Args:
            metadata: M√©tadonn√©es du chunk
            
        Returns:
            Titre format√©
        """
        title_parts = []
        
        # Ajoute le fichier source
        source_file = metadata.get("source_file", "Document")
        if source_file.endswith('.pdf'):
            source_file = source_file[:-4]  # Enl√®ve l'extension
        
        # Ajoute les informations structurelles
        if metadata.get("has_chapter") and metadata.get("chapter_title"):
            title_parts.append(f"Chapitre {metadata['chapter_title']}")
        
        if metadata.get("has_section") and metadata.get("section_title"):
            title_parts.append(f"Section {metadata['section_title']}")
        
        if metadata.get("has_article") and metadata.get("article_number"):
            title_parts.append(f"Article {metadata['article_number']}")
        
        # Construit le titre final
        if title_parts:
            structure = " - ".join(title_parts)
            return f"{source_file} - {structure}"
        else:
            return f"{source_file} - Page {metadata.get('page', 'N/A')}"
    
    def _clean_content(self, content: str) -> str:
        """
        Nettoie le contenu pour un meilleur affichage (r√®gles fran√ßaises)
        
        Args:
            content: Contenu brut
            
        Returns:
            Contenu nettoy√© avec ponctuation fran√ßaise correcte
        """
        import re
        
        # 1. Pr√©serve les retours √† la ligne simples, normalise les multiples
        # Ne pas tout mettre sur une ligne pour pr√©server la structure
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # 2. Supprime les espaces multiples sur une m√™me ligne
        content = re.sub(r'[ \t]+', ' ', content)
        
        # 3. G√®re la ponctuation fran√ßaise
        # Nettoie les espaces avant . et ,
        content = re.sub(r'\s+([.,])', r'\1', content)
        
        # Pr√©serve un seul espace avant : ; ! ?
        content = re.sub(r'\s*([;:!?])', r' \1', content)
        
        # Assure un espace apr√®s toute ponctuation
        content = re.sub(r'([.,;:!?])([A-Z√Ä-√öa-z√†-√∫0-9])', r'\1 \2', content)
        
        # 4. G√®re les guillemets fran√ßais
        content = re.sub(r'¬´\s*', '¬´ ', content)
        content = re.sub(r'\s*¬ª', ' ¬ª', content)
        
        # 5. Nettoie les espaces en d√©but/fin de lignes
        lines = [line.strip() for line in content.split('\n')]
        content = '\n'.join(line for line in lines if line)  # Enl√®ve les lignes vides
        
        # 6. Nettoie les espaces en d√©but et fin globaux
        content = content.strip()
        
        return content
    
    def _extract_context_info(self, metadata: dict) -> str:
        """
        Extrait les informations de contexte pertinentes
        
        Args:
            metadata: M√©tadonn√©es du chunk
            
        Returns:
            Informations de contexte format√©es
        """
        context_parts = []
        
        # Type de contenu
        content_type = metadata.get("content_type", "paragraph")
        if content_type == "article":
            context_parts.append("üìã Article r√©glementaire")
        elif content_type == "chapter":
            context_parts.append("üìñ Chapitre")
        elif content_type == "section":
            context_parts.append("üìë Section")
        else:
            context_parts.append("üìÑ Paragraphe")
        
        # Termes cl√©s (peuvent √™tre une cha√Æne ou une liste)
        key_terms = metadata.get("key_terms", [])
        if key_terms:
            if isinstance(key_terms, str):
                # Si c'est une cha√Æne, prend les 3 premiers termes
                terms_list = [term.strip() for term in key_terms.split(",")]
                terms_str = ", ".join(terms_list[:3])
            else:
                # Si c'est une liste
                terms_str = ", ".join(key_terms[:3])
            context_parts.append(f"üîë {terms_str}")
        
        return " ‚Ä¢ ".join(context_parts)


def main():
    """Fonction de test"""
    rag = RAGChain()
    
    # Question de test
    test_question = "Qu'est-ce que le RGPD et quels sont ses objectifs principaux ?"
    result = rag.query(test_question)
    
    print(f"\n{'='*60}")
    print(f"üìù R√©ponse:")
    print(f"{'='*60}")
    print(result["answer"])
    
    print(f"\n{'='*60}")
    print(f"üìö Sources utilis√©es:")
    print(f"{'='*60}")
    sources = rag.format_sources(result["source_documents"])
    for source in sources:
        print(f"\n[{source['index']}] {source['source_file']} - Page {source['page']}")
        print(f"Extrait: {source['content'][:200]}...")


if __name__ == "__main__":
    main()

