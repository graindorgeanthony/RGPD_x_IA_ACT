"""
ChaÃ®ne RAG pour interroger les documents RGPD et IA ACT
Utilise ChromaDB pour la recherche et Ollama pour la gÃ©nÃ©ration
"""
import os
import warnings

# Disable telemetry and warnings
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY"] = "False"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

# Suppress all common warnings
warnings.filterwarnings("ignore", message="Failed to send telemetry event")
warnings.filterwarnings("ignore", category=UserWarning, module="torch")
warnings.filterwarnings("ignore", message=".*torch.classes.*")

from typing import List, Dict, Any
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains.llm import LLMChain
from langchain_core.prompts import PromptTemplate
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.documents import Document


class StreamingCallbackHandler(BaseCallbackHandler):
    """Callback handler pour le streaming des rÃ©ponses"""
    
    def __init__(self, container):
        self.container = container
        self.current_text = ""
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """AppelÃ© Ã  chaque nouveau token gÃ©nÃ©rÃ©"""
        self.current_text += token
        # Met Ã  jour l'affichage en temps rÃ©el
        self.container.markdown(self.current_text)


class RAGChain:
    """Classe pour gÃ©rer la chaÃ®ne RAG"""
    
    def __init__(self, persist_directory: str = "./chroma_db", model: str = "gemma3:4b", num_sources: int = 5):
        """
        Initialise la chaÃ®ne RAG
        
        Args:
            persist_directory: RÃ©pertoire de la base vectorielle
            model: Nom du modÃ¨le Ollama Ã  utiliser
            num_sources: Nombre de sources Ã  rÃ©cupÃ©rer
        """
        self.persist_directory = persist_directory
        self.model_name = model
        self.num_sources = num_sources
        
        # Initialise les embeddings (mÃªmes que lors de l'indexation)
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # Charge la base vectorielle
        self.vectorstore = self._load_vectorstore()
        
        # Initialise le LLM local
        self.llm = self._init_llm()
        
        # CrÃ©e le retriever (pas de chaÃ®ne complÃ¨te, on gÃ¨re manuellement pour les citations)
        self.retriever = self._create_qa_chain()
    
    def _load_vectorstore(self) -> Chroma:
        """Charge la base vectorielle existante"""
        print(f"ğŸ“¦ Chargement de la base vectorielle depuis {self.persist_directory}")
        vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings
        )
        print(f"âœ… Base vectorielle chargÃ©e")
        return vectorstore
    
    def _init_llm(self) -> OllamaLLM:
        """Initialise le modÃ¨le LLM local via Ollama"""
        print(f"ğŸ¤– Initialisation du modÃ¨le {self.model_name} via Ollama")
        llm = OllamaLLM(
            model=self.model_name,
            temperature=0.1,  # TempÃ©rature basse pour plus de prÃ©cision
            callbacks=[StreamingStdOutCallbackHandler()]
        )
        print(f"âœ… ModÃ¨le initialisÃ©")
        return llm
    
    def _create_prompt_template(self) -> PromptTemplate:
        """CrÃ©e le template de prompt pour le LLM avec citations inline"""
        template = """Tu es un assistant expert en conformitÃ© juridique, spÃ©cialisÃ© dans le RGPD (RÃ¨glement GÃ©nÃ©ral sur la Protection des DonnÃ©es) et l'IA Act (rÃ¨glement europÃ©en sur l'intelligence artificielle).

RÃˆGLE ABSOLUE: RÃ©ponds PRÃ‰CISÃ‰MENT Ã  la question posÃ©e. Ne change pas de sujet.

Utilise UNIQUEMENT les informations du contexte ci-dessous pour rÃ©pondre Ã  la question. Si la rÃ©ponse n'est pas dans le contexte, dis clairement que tu ne peux pas rÃ©pondre avec certitude.

Contexte (extraits de documents juridiques):
{context}

Question: {question}

RAPPEL: Ta rÃ©ponse doit porter UNIQUEMENT sur: {question}

Instructions de rÃ©ponse IMPORTANTES:
- **STRUCTURE COMME UN EXECUTIVE SUMMARY** : Utilise des titres, sous-titres et une mise en forme claire
- RÃ©ponds de maniÃ¨re naturelle et fluide, comme un expert juridique
- **CRITICAL: Cite les sources Ã  la FIN de chaque paragraphe, pas dans le texte**
- **NE mentionne JAMAIS les sources dans le texte** (Ã©vite "Le texte [Source X] dit que...")
- Ã‰cris de maniÃ¨re directe et naturelle, puis ajoute [Source X] Ã  la fin du paragraphe
- Tu peux combiner plusieurs sources dans un mÃªme paragraphe: [Source 1] [Source 3]

STRUCTURE OBLIGATOIRE:
- Commence par un rÃ©sumÃ© en 2-3 phrases
- Utilise des **titres** (## Titre) pour organiser les sections principales
- Utilise des **sous-titres** (### Sous-titre) pour les dÃ©tails
- Fais des **listes Ã  puces** (-) quand c'est appropriÃ©
- Termine par une **conclusion** ou **recommandations** si pertinent
- Garde les paragraphes courts (3-4 phrases max)
- Cite les articles, chapitres et sections pertinents quand c'est possible
- Distingue clairement entre le RGPD et l'IA Act si les deux sont mentionnÃ©s
- Reste factuel et Ã©vite les interprÃ©tations personnelles
- Si la rÃ©ponse n'est pas dans le contexte, dis-le clairement

Format des citations:
- Place [Source X] Ã  la fin de chaque paragraphe qui utilise cette source
- Pour plusieurs sources dans un paragraphe: [Source 1] [Source 2]
- IMPORTANT: Ne cite JAMAIS les sources au milieu d'une phrase, TOUJOURS Ã  la fin du paragraphe

Exemples de MAUVAIS style (NE PAS FAIRE):
âŒ "Le texte [Source 7] stipule que..."
âŒ "Selon [Source 8], le DPO doit..."
âŒ "De plus, [Source 9] dÃ©crit..."

Exemples de BON style (Ã€ SUIVRE):
âœ… "Le responsable du traitement doit mettre en Å“uvre des mesures appropriÃ©es. [Source 7]"
âœ… "Il doit superviser les activitÃ©s de traitement et Ã©valuer les risques. [Source 3]"
âœ… "Les obligations incluent la supervision continue et l'Ã©valuation. [Source 1] [Source 3]"

RÃ‰PONDS UNIQUEMENT Ã€ LA QUESTION POSÃ‰E en utilisant le contexte fourni. Ne dÃ©vie pas du sujet. 

RÃ©ponse:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
    
    def _create_qa_chain(self):
        """
        CrÃ©e une chaÃ®ne RAG personnalisÃ©e avec citations numÃ©rotÃ©es
        Note: Retourne juste le retriever, on gÃ¨re manuellement la gÃ©nÃ©ration pour avoir plus de contrÃ´le
        """
        print(f"â›“ï¸  CrÃ©ation de la chaÃ®ne RAG avec support de citations")
        
        # On va gÃ©rer manuellement la chaÃ®ne pour avoir un contrÃ´le total sur le formatage
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": self.num_sources}
        )
        
        print(f"âœ… Retriever crÃ©Ã© avec support de citations numÃ©rotÃ©es")
        return self.retriever
    
    def _format_docs_with_sources(self, docs: List[Document]) -> str:
        """
        Formate les documents avec des numÃ©ros de source pour les citations
        
        Args:
            docs: Liste de documents rÃ©cupÃ©rÃ©s
            
        Returns:
            String formatÃ©e avec documents numÃ©rotÃ©s
        """
        formatted_docs = []
        for i, doc in enumerate(docs, 1):
            formatted_docs.append(f"[Source {i}]\n{doc.page_content}\n")
        return "\n".join(formatted_docs)
    
    def query(self, question: str) -> Dict[str, Any]:
        """
        Pose une question Ã  la chaÃ®ne RAG avec citations inline
        
        Args:
            question: La question Ã  poser
            
        Returns:
            Dictionnaire contenant la rÃ©ponse et les sources
        """
        print(f"\n{'='*60}")
        print(f"â“ Question: {question}")
        print(f"{'='*60}\n")
        
        # 1. RÃ©cupÃ¨re les documents pertinents (API moderne LangChain)
        print(f"ğŸ” Recherche de documents pertinents...")
        source_documents = self.retriever.invoke(question)
        print(f"âœ… {len(source_documents)} documents trouvÃ©s")
        
        # 2. Formate les documents avec numÃ©ros de source
        formatted_context = self._format_docs_with_sources(source_documents)
        
        # 3. CrÃ©e le prompt avec contexte numÃ©rotÃ©
        prompt_template = self._create_prompt_template()
        prompt = prompt_template.format(context=formatted_context, question=question)
        
        # 4. GÃ©nÃ¨re la rÃ©ponse avec le LLM
        print(f"ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec citations...")
        answer = self.llm.invoke(prompt)
        
        return {
            "question": question,
            "answer": answer,
            "source_documents": source_documents
        }
    
    def query_streaming(self, question: str, container) -> Dict[str, Any]:
        """
        Pose une question Ã  la chaÃ®ne RAG avec streaming en temps rÃ©el
        
        Args:
            question: La question Ã  poser
            container: Container Streamlit pour l'affichage en temps rÃ©el
            
        Returns:
            Dictionnaire contenant la rÃ©ponse et les sources
        """
        print(f"\n{'='*60}")
        print(f"â“ Question (streaming): {question}")
        print(f"{'='*60}\n")
        
        # 1. RÃ©cupÃ¨re les documents pertinents
        print(f"ğŸ” Recherche de documents pertinents...")
        source_documents = self.retriever.invoke(question)
        print(f"âœ… {len(source_documents)} documents trouvÃ©s")
        
        # 2. Formate les documents avec numÃ©ros de source
        formatted_context = self._format_docs_with_sources(source_documents)
        
        # 3. CrÃ©e le prompt avec contexte numÃ©rotÃ©
        prompt_template = self._create_prompt_template()
        prompt = prompt_template.format(context=formatted_context, question=question)
        
        # 4. Configure le LLM avec le callback de streaming
        streaming_callback = StreamingCallbackHandler(container)
        llm_with_streaming = OllamaLLM(
            model=self.model_name,
            temperature=0.1,
            callbacks=[streaming_callback]
        )
        
        # 5. GÃ©nÃ¨re la rÃ©ponse avec streaming
        print(f"ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec streaming...")
        answer = llm_with_streaming.invoke(prompt)
        
        return {
            "question": question,
            "answer": answer,
            "source_documents": source_documents
        }
    
    def extract_citations(self, answer: str) -> Dict[str, Any]:
        """
        Extrait les citations [Source X] de la rÃ©ponse et crÃ©e un mapping
        
        Args:
            answer: RÃ©ponse gÃ©nÃ©rÃ©e par le LLM
            
        Returns:
            Dict avec la rÃ©ponse et les sources citÃ©es
        """
        import re
        
        # Trouve toutes les citations [Source X] dans la rÃ©ponse
        citation_pattern = r'\[Source\s+(\d+)\]'
        citations = re.findall(citation_pattern, answer)
        
        # Convertit en set pour avoir les sources uniques, puis en liste triÃ©e
        cited_sources = sorted(set(int(c) for c in citations))
        
        return {
            "answer": answer,
            "cited_sources": cited_sources,
            "total_citations": len(citations)
        }
    
    def format_sources(self, source_documents: List) -> List[Dict[str, str]]:
        """
        Formate les documents sources pour l'affichage avec mÃ©tadonnÃ©es enrichies
        
        Args:
            source_documents: Liste de documents sources
            
        Returns:
            Liste de dictionnaires formatÃ©s avec informations structurelles
        """
        sources = []
        for i, doc in enumerate(source_documents):
            metadata = doc.metadata
            
            # Construit le titre du chunk basÃ© sur la structure
            chunk_title = self._build_chunk_title(metadata)
            
            # Nettoie le contenu pour un meilleur affichage
            cleaned_content = self._clean_content(doc.page_content)
            
            # Extrait les informations de contexte
            context_info = self._extract_context_info(metadata)
            
            sources.append({
                "index": i + 1,
                "content": cleaned_content,
                "source_file": metadata.get("source_file", "Inconnu"),
                "page": metadata.get("page", "N/A"),
                "chunk_title": chunk_title,
                "content_type": metadata.get("content_type", "paragraph"),
                "article_number": metadata.get("article_number"),
                "chapter_title": metadata.get("chapter_title"),
                "section_title": metadata.get("section_title"),
                "key_terms": metadata.get("key_terms", []),
                "word_count": metadata.get("word_count", 0),
                "context_info": context_info,
                "chunk_quality": metadata.get("chunk_quality", 1.0),
                "is_complete": metadata.get("is_complete", True)
            })
        return sources
    
    def _build_chunk_title(self, metadata: dict) -> str:
        """
        Construit un titre descriptif pour le chunk
        
        Args:
            metadata: MÃ©tadonnÃ©es du chunk
            
        Returns:
            Titre formatÃ©
        """
        title_parts = []
        
        # Ajoute le fichier source
        source_file = metadata.get("source_file", "Document")
        if source_file.endswith('.pdf'):
            source_file = source_file[:-4]  # EnlÃ¨ve l'extension
        
        # Ajoute les informations structurelles
        if metadata.get("has_chapter") and metadata.get("chapter_title"):
            title_parts.append(f"Chapitre {metadata['chapter_title']}")
        
        if metadata.get("has_section") and metadata.get("section_title"):
            title_parts.append(f"Section {metadata['section_title']}")
        
        if metadata.get("has_article") and metadata.get("article_number"):
            title_parts.append(f"Article {metadata['article_number']}")
        
        # Construit le titre final
        if title_parts:
            structure = " - ".join(title_parts)
            return f"{source_file} - {structure}"
        else:
            return f"{source_file} - Page {metadata.get('page', 'N/A')}"
    
    def _clean_content(self, content: str) -> str:
        """
        Nettoie le contenu pour un meilleur affichage (rÃ¨gles franÃ§aises)
        
        Args:
            content: Contenu brut
            
        Returns:
            Contenu nettoyÃ© avec ponctuation franÃ§aise correcte
        """
        import re
        
        # 1. PrÃ©serve les retours Ã  la ligne simples, normalise les multiples
        # Ne pas tout mettre sur une ligne pour prÃ©server la structure
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # 2. Supprime les espaces multiples sur une mÃªme ligne
        content = re.sub(r'[ \t]+', ' ', content)
        
        # 3. GÃ¨re la ponctuation franÃ§aise
        # Nettoie les espaces avant . et ,
        content = re.sub(r'\s+([.,])', r'\1', content)
        
        # PrÃ©serve un seul espace avant : ; ! ?
        content = re.sub(r'\s*([;:!?])', r' \1', content)
        
        # Assure un espace aprÃ¨s toute ponctuation
        content = re.sub(r'([.,;:!?])([A-ZÃ€-Ãša-zÃ -Ãº0-9])', r'\1 \2', content)
        
        # 4. GÃ¨re les guillemets franÃ§ais
        content = re.sub(r'Â«\s*', 'Â« ', content)
        content = re.sub(r'\s*Â»', ' Â»', content)
        
        # 5. Nettoie les espaces en dÃ©but/fin de lignes
        lines = [line.strip() for line in content.split('\n')]
        content = '\n'.join(line for line in lines if line)  # EnlÃ¨ve les lignes vides
        
        # 6. Nettoie les espaces en dÃ©but et fin globaux
        content = content.strip()
        
        return content
    
    def _extract_context_info(self, metadata: dict) -> str:
        """
        Extrait les informations de contexte pertinentes
        
        Args:
            metadata: MÃ©tadonnÃ©es du chunk
            
        Returns:
            Informations de contexte formatÃ©es
        """
        context_parts = []
        
        # Type de contenu
        content_type = metadata.get("content_type", "paragraph")
        if content_type == "article":
            context_parts.append("ğŸ“‹ Article rÃ©glementaire")
        elif content_type == "chapter":
            context_parts.append("ğŸ“– Chapitre")
        elif content_type == "section":
            context_parts.append("ğŸ“‘ Section")
        else:
            context_parts.append("ğŸ“„ Paragraphe")
        
        # Termes clÃ©s (peuvent Ãªtre une chaÃ®ne ou une liste)
        key_terms = metadata.get("key_terms", [])
        if key_terms:
            if isinstance(key_terms, str):
                # Si c'est une chaÃ®ne, prend les 3 premiers termes
                terms_list = [term.strip() for term in key_terms.split(",")]
                terms_str = ", ".join(terms_list[:3])
            else:
                # Si c'est une liste
                terms_str = ", ".join(key_terms[:3])
            context_parts.append(f"ğŸ”‘ {terms_str}")
        
        return " â€¢ ".join(context_parts)


def main():
    """Fonction de test"""
    rag = RAGChain()
    
    # Question de test
    test_question = "Qu'est-ce que le RGPD et quels sont ses objectifs principaux ?"
    result = rag.query(test_question)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“ RÃ©ponse:")
    print(f"{'='*60}")
    print(result["answer"])
    
    print(f"\n{'='*60}")
    print(f"ğŸ“š Sources utilisÃ©es:")
    print(f"{'='*60}")
    sources = rag.format_sources(result["source_documents"])
    for source in sources:
        print(f"\n[{source['index']}] {source['source_file']} - Page {source['page']}")
        print(f"Extrait: {source['content'][:200]}...")


if __name__ == "__main__":
    main()

