"""
Configuration centralis√©e pour l'application RGPD + IA ACT
Modifiez ce fichier pour personnaliser l'application
"""

# =============================================================================
# Configuration du Mod√®le LLM
# =============================================================================

# Mod√®le Ollama √† utiliser
# Options populaires : "gemma3:4b", "gemma2:2b", "llama3:8b", "mistral:7b"
LLM_MODEL = "gemma3:4b"

# Temp√©rature du mod√®le (0.0 = tr√®s factuel, 1.0 = tr√®s cr√©atif)
LLM_TEMPERATURE = 0.1

# URL du serveur Ollama (modifier si Ollama est sur une autre machine)
OLLAMA_BASE_URL = "http://localhost:11434"


# =============================================================================
# Configuration de l'Embedding
# =============================================================================

# Mod√®le d'embedding √† utiliser
# Options : "sentence-transformers/all-MiniLM-L6-v2" (rapide, l√©ger)
#           "sentence-transformers/all-mpnet-base-v2" (meilleur, plus lourd)
#           "sentence-transformers/all-MiniLM-L12-v2" (√©quilibre performance/taille)
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# Device pour les embeddings ("cpu" ou "cuda")
EMBEDDING_DEVICE = "cpu"


# =============================================================================
# Configuration de la Base Vectorielle
# =============================================================================

# R√©pertoire de stockage de ChromaDB
CHROMA_PERSIST_DIRECTORY = "./chroma_db"

# Nombre de chunks √† r√©cup√©rer pour chaque requ√™te
RETRIEVAL_K = 5


# =============================================================================
# Configuration du D√©coupage de Documents
# =============================================================================

# Taille des chunks en caract√®res (augment√© pour pr√©server les listes compl√®tes)
CHUNK_SIZE = 1500

# Chevauchement entre chunks en caract√®res (augment√© pour meilleure continuit√©)
# Un overlap √©lev√© (30-40%) aide √† pr√©server le contexte des √©num√©rations
CHUNK_OVERLAP = 500

# Note: Les s√©parateurs sont g√©r√©s dynamiquement dans l'indexer avec regex
# pour mieux d√©tecter les structures juridiques (√©num√©rations a), b), c), etc.)
CHUNK_SEPARATORS = ["\n\n", "\n", ".", "!", "?", " ", ""]


# =============================================================================
# Configuration des Donn√©es
# =============================================================================

# R√©pertoire contenant les PDFs source
KNOWLEDGE_BASE_DIR = "./knowledge_base"

# Extensions de fichiers √† indexer
SUPPORTED_EXTENSIONS = [".pdf"]


# =============================================================================
# Configuration de l'Interface Streamlit
# =============================================================================

# Titre de la page
APP_TITLE = "Assistant de Conformit√© RGPD + IA ACT"

# Ic√¥ne de la page (emoji)
APP_ICON = "üõ°Ô∏è"

# Layout ("centered" ou "wide")
APP_LAYOUT = "wide"

# Nombre maximum de questions dans l'historique
MAX_HISTORY = 10

# Afficher les sources par d√©faut
DEFAULT_SHOW_SOURCES = True

# Nombre de sources √† afficher par d√©faut
DEFAULT_NUM_SOURCES = 5


# =============================================================================
# Configuration du Prompt
# =============================================================================

# Template de prompt pour le LLM
PROMPT_TEMPLATE = """Tu es un assistant expert en conformit√© juridique, sp√©cialis√© dans le RGPD (R√®glement G√©n√©ral sur la Protection des Donn√©es) et l'IA Act (r√®glement europ√©en sur l'intelligence artificielle).

Utilise UNIQUEMENT les informations du contexte ci-dessous pour r√©pondre √† la question. Si la r√©ponse n'est pas dans le contexte, dis clairement que tu ne peux pas r√©pondre avec certitude.

Contexte:
{context}

Question: {question}

Instructions:
- R√©ponds de mani√®re pr√©cise et structur√©e
- Cite les articles pertinents quand c'est possible
- Si plusieurs documents sont pertinents (RGPD et IA Act), indique clairement de quel r√®glement tu parles
- Reste factuel et √©vite les interpr√©tations
- Si la r√©ponse n'est pas dans le contexte, dis-le clairement

R√©ponse:"""


# =============================================================================
# Configuration du Logging
# =============================================================================

# Niveau de log ("DEBUG", "INFO", "WARNING", "ERROR")
LOG_LEVEL = "INFO"

# Activer les logs d√©taill√©s
VERBOSE_LOGGING = False


# =============================================================================
# Questions Sugg√©r√©es
# =============================================================================

SUGGESTED_QUESTIONS = {
    "RGPD": [
        "Quels sont les droits des personnes concern√©es selon le RGPD ?",
        "Quel est le r√¥le et les responsabilit√©s du DPO ?",
        "Quelles sont les sanctions en cas de non-conformit√© RGPD ?",
        "Qu'est-ce qu'une analyse d'impact relative √† la protection des donn√©es ?",
        "Quels sont les principes de traitement des donn√©es personnelles ?",
    ],
    "IA Act": [
        "Quelles sont les cat√©gories de risques selon l'IA Act ?",
        "Quelles sont les pratiques d'IA interdites ?",
        "Qu'est-ce qu'un syst√®me d'IA √† haut risque ?",
        "Quelles sont les obligations pour les fournisseurs de syst√®mes d'IA ?",
        "Comment l'IA Act d√©finit-il un syst√®me d'intelligence artificielle ?",
    ],
    "Transversal": [
        "Comment le RGPD et l'IA Act interagissent-ils en mati√®re de protection des donn√©es ?",
        "Quelles sont les exigences communes entre le RGPD et l'IA Act ?",
    ]
}


# =============================================================================
# Fonctions Utilitaires
# =============================================================================

def get_config_summary():
    """Retourne un r√©sum√© de la configuration actuelle"""
    return {
        "LLM": {
            "Mod√®le": LLM_MODEL,
            "Temp√©rature": LLM_TEMPERATURE,
        },
        "Embedding": {
            "Mod√®le": EMBEDDING_MODEL,
            "Device": EMBEDDING_DEVICE,
        },
        "Retrieval": {
            "K chunks": RETRIEVAL_K,
        },
        "Chunking": {
            "Taille": CHUNK_SIZE,
            "Overlap": CHUNK_OVERLAP,
        },
    }


if __name__ == "__main__":
    # Afficher la configuration actuelle
    import json
    print("Configuration actuelle :")
    print(json.dumps(get_config_summary(), indent=2, ensure_ascii=False))

